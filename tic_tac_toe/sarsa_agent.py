import random
from collections import defaultdict
from functools import partial
from typing import Dict, Tuple, List

import numpy as np

from tic_tac_toe.tic_tac_toe_game import Position, TTTAction, TTTState


QVALType = Dict[Tuple, Dict[TTTAction, float]]


class SarsaAgent(object):
    def __init__(self,
                 marker: int=1,
                 epsilon: float=0.1,
                 gamma: float=0.9,
                 alpha: float=0.9,
                 lambda_: float=0.9,
                 random_seed: int=10) -> None:
        # TODO: Check all parameters are sensible
        self.marker = marker
        self.eligibility: Dict[Tuple, float] = defaultdict(float)
        # TODO: Analyse action values - pick out ones which are clearly 'good' because
        # they are 1 away from winning
        self.q_values: QVALType = defaultdict(dict)  # {state1: {action1: value}, state2: {action2: value}}
        self.epsilon = epsilon
        self.gamma = gamma
        self.alpha = alpha
        self.lambda_ = lambda_
        self._random = random.Random(random_seed)
        self.episode_number = 1

    @property
    def alpha_k(self):
        return self.alpha / np.sqrt(self.episode_number)

    @property
    def epsilon_k(self):
        return self.epsilon / np.sqrt(self.episode_number)

    def act(self, state: TTTState, available_positions: List[Position]) -> TTTAction:
        if state.is_terminal:
            return TTTAction(self.marker, Position(-1, -1))  # Sentinel
        action = self.choose_egreedy(state, available_positions)
        hashable_state = self.hashable_state(state)
        if action not in self.q_values.get(hashable_state, {}):
            # Make sure this action is in the table so it gets updated
            # in the self.get_feedback call
            self.q_values[hashable_state][action] = 0.0
        return action

    def get_feedback(self,
                     state: TTTState,
                     action: TTTAction,
                     reward: int,
                     new_state: TTTState,
                     new_action: TTTAction) -> None:
        # TODO: Check td_error calc
        td_error = (reward
                    + self.gamma * self.get_action_value(new_state, new_action)
                    - self.get_action_value(state, action))
        # TODO: supposed to update eligibility for last state?
        self.eligibility[(self.hashable_state(state), action)] += 1.0
        self.update_q_values(td_error)
        self.update_eligibility_traces()

    def update_eligibility_traces(self) -> None:
        # TODO: Unittest
        new_eligibility: Dict[Tuple, float] = defaultdict(float)
        for eligibilitykey, eligibilityvalue in self.eligibility.items():
            new_eligibility[eligibilitykey] = self.gamma * self.lambda_ * eligibilityvalue
        self.eligibility = new_eligibility

    def update_q_values(self, td_error: float) -> None:
        # TODO: Unittest
        new_qvals: QVALType = defaultdict(dict)
        for statekey, action_value_map in self.q_values.items():
            for actionkey, actionvalue in action_value_map.items():
                new_qvals[statekey][actionkey] = (
                    actionvalue + self.alpha_k * td_error * self.eligibility[(statekey, actionkey)])
        self.q_values = new_qvals

    def reset(self):
        self.eligibility.clear()
        self.episode_number += 1

    def choose_egreedy(self, state: TTTState, available_positions: List[Position]) -> TTTAction:
        # TODO: This "possible actions" should be generated by the game
        possible_actions = [TTTAction(self.marker, pos) for pos in available_positions]
        if self.should_explore():
            choice = self._random.choice(possible_actions)
        else:
            self._random.shuffle(possible_actions)
            choice = self.exploitative_action(possible_actions, state)
        return choice

    def exploitative_action(self,
                            possible_actions: List[TTTAction],
                            state: TTTState) -> TTTAction:
        # TODO: Unittest with known action values
        choice = max(possible_actions, key=partial(self.get_action_value,
                                                   state))
        return choice

    def get_action_value(self, state: TTTState, action: TTTAction) -> int:
        if state.is_terminal:
            return 0
        hstate = self.hashable_state(state)
        return self.q_values[hstate].get(action, 0)

    def hashable_state(self, state: TTTState) -> Tuple[int]:
        return tuple(state.board.flatten())

    def should_explore(self) -> bool:
        return self._random.random() < self.epsilon_k